{
    "docs": [
        {
            "location": "/", 
            "text": "DataRescue Workflow\n\n\nThis document describes DataRescue activities both at in-person events and remotely, as developed by the \nDataRefuge project\n and \nEDGI\n. It explains the process that a URL/dataset goes through from the time it has been identified, either by a \nSeeder\n as difficult to preserve, or  \"uncrawlable,\" until it is made available as a record in the \ndatarefuge.org\n data catalog. The process involves several stages and is designed to maximize smooth hand-offs. At each step the data is with someone with distinct expertise and the data is always being tracked for security.\n\n\n\n\nEvent Organizers\n\n\nLearn about what you need to do to \nbefore\n and \nafter\n an event.\n\n\nEvent Attendees\n\n\n\n\nJoin the event Slack team recommended by event organizers, this is often the \nDataRefuge Slack\n. During the event people share expertise and answer each other's questions here.  \n\n\nPick your role from the paths below, get account credentials, and make sure you have access to the key documents and tools you need to work. Organizers will instruct you on these steps.\n\n\nReview the relevant sections(s) of this workflow.\n\n\n\n\nPath I. Surveying\n\n\nSurveying\n\n\nSurveyors identify key programs, datasets, and documents on Federal Agency websites that are vulnerable to change and loss. Using templates and how-to guides, they create Main Agency Primers in order to introduce a particular agency, and Sub-Agency Primers in order to guide web archiving efforts by laying out a list of URLs that cover the breadth of an office.\n\n\nPath II. Website Archiving\n\n\nSeeding\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the \nInternet Archive's\n web crawler. Using the \nEDGI Nomination Chrome extension\n, Seeders nominate crawlable URLs to the Internet Archive or add them to the Archivers app if they require manual archiving.\n\n\nPath III. Archiving More Complex Datasets\n\n\nA. \nResearching\n\n\nResearchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested. \nResearching.md\n describes this process in more detail.\n\n\nWe recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.\n\n\nB. \nHarvesting\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise and different techniques for different tasks. Harvesters should also review the \nHarvesting Toolkit\n for tools.\n\n\nC. \nChecking/Bagging\n\n\nCheckers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets. \n \nNote: Checking is currently performed by Baggers and does not exist as a separate stage in the Archivers app.\n\n\nBaggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\nD. \nDescribing\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.\n\n\n\n\nPartners\n\n\nDataRescue is a broad, grassroots effort with support from numerous local and nationwide networks. \nDataRefuge\n and \nEDGI\n partner with local organizers in supporting these events. See more of our institutional partners on the \nDataRefuge home page\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#datarescue-workflow", 
            "text": "This document describes DataRescue activities both at in-person events and remotely, as developed by the  DataRefuge project  and  EDGI . It explains the process that a URL/dataset goes through from the time it has been identified, either by a  Seeder  as difficult to preserve, or  \"uncrawlable,\" until it is made available as a record in the  datarefuge.org  data catalog. The process involves several stages and is designed to maximize smooth hand-offs. At each step the data is with someone with distinct expertise and the data is always being tracked for security.", 
            "title": "DataRescue Workflow"
        }, 
        {
            "location": "/#event-organizers", 
            "text": "Learn about what you need to do to  before  and  after  an event.", 
            "title": "Event Organizers"
        }, 
        {
            "location": "/#event-attendees", 
            "text": "Join the event Slack team recommended by event organizers, this is often the  DataRefuge Slack . During the event people share expertise and answer each other's questions here.    Pick your role from the paths below, get account credentials, and make sure you have access to the key documents and tools you need to work. Organizers will instruct you on these steps.  Review the relevant sections(s) of this workflow.", 
            "title": "Event Attendees"
        }, 
        {
            "location": "/#path-i-surveying", 
            "text": "", 
            "title": "Path I. Surveying"
        }, 
        {
            "location": "/#surveying", 
            "text": "Surveyors identify key programs, datasets, and documents on Federal Agency websites that are vulnerable to change and loss. Using templates and how-to guides, they create Main Agency Primers in order to introduce a particular agency, and Sub-Agency Primers in order to guide web archiving efforts by laying out a list of URLs that cover the breadth of an office.", 
            "title": "Surveying"
        }, 
        {
            "location": "/#path-ii-website-archiving", 
            "text": "", 
            "title": "Path II. Website Archiving"
        }, 
        {
            "location": "/#seeding", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the  Internet Archive's  web crawler. Using the  EDGI Nomination Chrome extension , Seeders nominate crawlable URLs to the Internet Archive or add them to the Archivers app if they require manual archiving.", 
            "title": "Seeding"
        }, 
        {
            "location": "/#path-iii-archiving-more-complex-datasets", 
            "text": "", 
            "title": "Path III. Archiving More Complex Datasets"
        }, 
        {
            "location": "/#a-researching", 
            "text": "Researchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.  Researching.md  describes this process in more detail.  We recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.", 
            "title": "A. Researching"
        }, 
        {
            "location": "/#b-harvesting", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise and different techniques for different tasks. Harvesters should also review the  Harvesting Toolkit  for tools.", 
            "title": "B. Harvesting"
        }, 
        {
            "location": "/#c-checkingbagging", 
            "text": "Checkers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.    Note: Checking is currently performed by Baggers and does not exist as a separate stage in the Archivers app.  Baggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.", 
            "title": "C. Checking/Bagging"
        }, 
        {
            "location": "/#d-describing", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.", 
            "title": "D. Describing"
        }, 
        {
            "location": "/#partners", 
            "text": "DataRescue is a broad, grassroots effort with support from numerous local and nationwide networks.  DataRefuge  and  EDGI  partner with local organizers in supporting these events. See more of our institutional partners on the  DataRefuge home page .", 
            "title": "Partners"
        }, 
        {
            "location": "/surveying/", 
            "text": "What Do Surveyors Do?\n\n\nSurveyors identify key programs, datasets, and documents on Federal Agency websites that are vulnerable to change and loss. Using templates and how-to guides, they create Main Agency Primers in order to introduce a particular agency, and Sub-Agency Primers in order to guide web archiving efforts by laying out a list of URLs that cover the breadth of an office.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you\u2019re familiar with federal data, interested in particular offices or data sets that don\u2019t already have a primer, or want to help create materials for use at other archiving events!\n\n\n\n\nGetting Set up as a Surveyor\n\n\nAnatomy of a Primer\n\n\nFor each federal agency website, we create a set of Agency Primers that consist of a Main Agency Primer and several Sub-Agency Primers.\n\n\nMain Agency Primers (MAPs)\n describe the range of risks to the listed agencies\u2019 current programming in the coming administration, focusing on offices with programs relating to climate change, renewable energy, sustainability, and the environment. Each agency or department has one MAP. MAPs are not used for web archiving, instead they are used as introductory documents to become familiar with a particular agency.\n\n\n\n  \nSee Example MAP: \nDepartment of Energy\n\n\n\n\n\nSub-Agency Primers (SAPs)\n identify at-risk offices within agencies and guide the web archiving efforts. A particular agency or department can have several SAPs, one for each office. The list of SAPs is linked in the MAP for a particular department or agency.\n\n\n\n  \nSee Example SAP: \nDepartment of Energy - Office of Energy Efficiency and Renewable Energy \n\n\n\n\n\nThe full list of completed Agency Archiving Primers is available on \nEDGI\u2019s website\n. Our \nGoogle Drive\n has primers in the process of being prepared for use.\n\n\nClaiming a Primer\n\n\n\n\nIdentify a particular office you\u2019d like to work with.\n\n\nCheck the Agency Office Database for the status of the primer for that office. The possible statuses for a primer are:\n\n\nNo primer created for the office yet\n\nclaim the primer and start writing!\n\n\nA primer has been written but not checked\n\nclaim the primer and start checking!\n\n\nA primer has been written and checked\n\nclaim the primer and start web archiving!\n\n\n\n\n\n\nClaim the primer for a specific purpose (writing, checking, or using) through the \nAdd/Edit a Primer form\n.\n\n\n\n\nCreating or Checking a New Primer\n\n\n\n\nClaim the primer for a specific office using the \nAdd/Edit a Primer form\n.\n\n\nNavigate to the Agency Primers \nGoogle Drive\n.\n\n\n\n\nIf there is \nno existing primer\n for that office, follow MAP or SAP instructions below:\n\n\nMAP Path\n\n\n\n\nCreate a MAP for a main agency by copying the \nMAP template\n and saving it to the agency folder (ex. DOJ folder).\n\n\nFollow the \nPrimer How-To\n (look for the MAP section) for guidance.\n\n\n\n\nSAP Path\n\n\n\n\nCreate SAP for individual office or division by copying the \nSAP template\n and saving the copy to the agency folder (ex. Bureau of Land Management \nin\n DOI folder).\n\n\nFollow the \nPrimer How-To\n (look for the SAP section) for guidance.\n\n\n\n\n\n\n\n\nIf there is an \nexisting primer\n that needs to be checked:\n\nWork with others from the agency\u2019s designated community (e.g. experts from communities, librarians, professional associations) who can provide input on vulnerable or at risk information.\n\n\n\n\nWhen you\u2019re done, update the Agency Office Database status and primer link by again submitting the \nAdd/Edit a Primer form\n.", 
            "title": "Surveying"
        }, 
        {
            "location": "/surveying/#what-do-surveyors-do", 
            "text": "Surveyors identify key programs, datasets, and documents on Federal Agency websites that are vulnerable to change and loss. Using templates and how-to guides, they create Main Agency Primers in order to introduce a particular agency, and Sub-Agency Primers in order to guide web archiving efforts by laying out a list of URLs that cover the breadth of an office.  \n   Recommended Skills     \n  Consider this path if you\u2019re familiar with federal data, interested in particular offices or data sets that don\u2019t already have a primer, or want to help create materials for use at other archiving events!", 
            "title": "What Do Surveyors Do?"
        }, 
        {
            "location": "/surveying/#getting-set-up-as-a-surveyor", 
            "text": "", 
            "title": "Getting Set up as a Surveyor"
        }, 
        {
            "location": "/surveying/#anatomy-of-a-primer", 
            "text": "For each federal agency website, we create a set of Agency Primers that consist of a Main Agency Primer and several Sub-Agency Primers.  Main Agency Primers (MAPs)  describe the range of risks to the listed agencies\u2019 current programming in the coming administration, focusing on offices with programs relating to climate change, renewable energy, sustainability, and the environment. Each agency or department has one MAP. MAPs are not used for web archiving, instead they are used as introductory documents to become familiar with a particular agency.  \n   See Example MAP:  Department of Energy   Sub-Agency Primers (SAPs)  identify at-risk offices within agencies and guide the web archiving efforts. A particular agency or department can have several SAPs, one for each office. The list of SAPs is linked in the MAP for a particular department or agency.  \n   See Example SAP:  Department of Energy - Office of Energy Efficiency and Renewable Energy    The full list of completed Agency Archiving Primers is available on  EDGI\u2019s website . Our  Google Drive  has primers in the process of being prepared for use.", 
            "title": "Anatomy of a Primer"
        }, 
        {
            "location": "/surveying/#claiming-a-primer", 
            "text": "Identify a particular office you\u2019d like to work with.  Check the Agency Office Database for the status of the primer for that office. The possible statuses for a primer are:  No primer created for the office yet \nclaim the primer and start writing!  A primer has been written but not checked \nclaim the primer and start checking!  A primer has been written and checked \nclaim the primer and start web archiving!    Claim the primer for a specific purpose (writing, checking, or using) through the  Add/Edit a Primer form .", 
            "title": "Claiming a Primer"
        }, 
        {
            "location": "/surveying/#creating-or-checking-a-new-primer", 
            "text": "Claim the primer for a specific office using the  Add/Edit a Primer form .  Navigate to the Agency Primers  Google Drive .   If there is  no existing primer  for that office, follow MAP or SAP instructions below:  MAP Path   Create a MAP for a main agency by copying the  MAP template  and saving it to the agency folder (ex. DOJ folder).  Follow the  Primer How-To  (look for the MAP section) for guidance.   SAP Path   Create SAP for individual office or division by copying the  SAP template  and saving the copy to the agency folder (ex. Bureau of Land Management  in  DOI folder).  Follow the  Primer How-To  (look for the SAP section) for guidance.     If there is an  existing primer  that needs to be checked: \nWork with others from the agency\u2019s designated community (e.g. experts from communities, librarians, professional associations) who can provide input on vulnerable or at risk information.   When you\u2019re done, update the Agency Office Database status and primer link by again submitting the  Add/Edit a Primer form .", 
            "title": "Creating or Checking a New Primer"
        }, 
        {
            "location": "/seeding/", 
            "text": "What Do Seeders Do?\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the \nInternet Archive's\n web crawler. Using the \nEDGI Nomination Chrome extension\n, Seeders nominate crawlable URLs to the Internet Archive or add them to the Archivers app if they require manual archiving.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.\n\n\n\n\nChoosing the Website\n\n\nSeeders use the \nEDGI Archiving Primers\n, or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.\n\n\nCanvassing the Website and Evaluating Content\n\n\n\n\nStart exploring the website assigned, identifying important URLs.\n\n\nDecide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.\n\n\nEDGI's Guides\n have information critical to the seeding and sorting process:\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\n\n\n\n\nCrawlable URLs\n\n\n\n\nURLs judged to be crawlable are nominated (\"seeded\") to the Internet Archive, using the \nEDGI Nomination Chrome extension\n.\n\n\n\n\nWherever possible, add in the Agency Office Code from the sub-primer.\n Talk to the DataRescue organizers to learn more.\n\n\nUncrawlable URLs\n\n\n\n\nIf URL is judged not crawlable, check one of the checkboxes next to the four types of uncrawlables in the Chrome Extension. This will add the URL to the Researching queue in the Archivers app.\n\n\nThe URL will be automatically associated with a universal unique identifier (UUID).\n\n\nYou can check whether the page or some files are archived using the Internet Archive's \nWayback Machine Chrome Extension\n\n\n\n\nNot Sure?\n\n\n\n\nThis sorting is only provisional; when in doubt, Seeders nominate the URL \nand\n mark it as possibly not crawlable.", 
            "title": "Seeding"
        }, 
        {
            "location": "/seeding/#what-do-seeders-do", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the  Internet Archive's  web crawler. Using the  EDGI Nomination Chrome extension , Seeders nominate crawlable URLs to the Internet Archive or add them to the Archivers app if they require manual archiving.  \n   Recommended Skills     \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.", 
            "title": "What Do Seeders Do?"
        }, 
        {
            "location": "/seeding/#choosing-the-website", 
            "text": "Seeders use the  EDGI Archiving Primers , or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.", 
            "title": "Choosing the Website"
        }, 
        {
            "location": "/seeding/#canvassing-the-website-and-evaluating-content", 
            "text": "Start exploring the website assigned, identifying important URLs.  Decide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.  EDGI's Guides  have information critical to the seeding and sorting process:  Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler", 
            "title": "Canvassing the Website and Evaluating Content"
        }, 
        {
            "location": "/seeding/#crawlable-urls", 
            "text": "URLs judged to be crawlable are nominated (\"seeded\") to the Internet Archive, using the  EDGI Nomination Chrome extension .   Wherever possible, add in the Agency Office Code from the sub-primer.  Talk to the DataRescue organizers to learn more.", 
            "title": "Crawlable URLs"
        }, 
        {
            "location": "/seeding/#uncrawlable-urls", 
            "text": "If URL is judged not crawlable, check one of the checkboxes next to the four types of uncrawlables in the Chrome Extension. This will add the URL to the Researching queue in the Archivers app.  The URL will be automatically associated with a universal unique identifier (UUID).  You can check whether the page or some files are archived using the Internet Archive's  Wayback Machine Chrome Extension", 
            "title": "Uncrawlable URLs"
        }, 
        {
            "location": "/seeding/#not-sure", 
            "text": "This sorting is only provisional; when in doubt, Seeders nominate the URL  and  mark it as possibly not crawlable.", 
            "title": "Not Sure?"
        }, 
        {
            "location": "/researching/", 
            "text": "What Do Researchers Do?\n\n\nResearchers review \"uncrawlables\" identified during \nSeeding\n, confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.\n\n\n\n\nGetting Set up as a Researcher\n\n\n\n\nEvent organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.\n\n\nThey will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event\n\n\nOr post questions in the DataRefuge Slack \n#researchers\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\n\n  \nResearchers and Harvesters\n \n  \n  \n\n    \nResearchers and Harvesters should coordinate together as their work is closely related and benefits from close communication\n\n    \nIt may be most effective to work together in pairs or small groups, or for a single person to both research and harvest\n\n    \nAs a Researcher, make sure to check out the \nHarvesting documentation\n to familiarize yourself with their role\n\n  \n\n\n\n\n\nClaiming a Dataset to Research\n\n\n\n  \nUsing Archivers App\n \n  \n  Review our walkthrough video below and refer to the \nFAQ\n for any additional questions about the \nArchivers app\n. \n\n  \n\n  \n\n\n\n\n\n\n\nResearchers work on datasets that were listed as uncrawlable by Seeders.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nRESEARCH\n: all the URLs listed are ready to be researched.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\nPriority is indicated by the \u201c!\u201d field.  The range is from 0 to 10, with 10 being highest priority.\n\n\n\n\n\n\nSelect an available URL (you may decide to select a URL relevant to your area of expertise or assigned a high priority) and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nURL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nEvaluating the Data\n\n\nGo to the URL and start inspecting the content.\n\n\nIs the data actually crawlable?\n\n\nAgain, see \nEDGI's Guides\n for a mostly non-technical introduction to the crawler:\n\n\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\nSome additional technical notes for answering this:\n\n\n\n\nThere is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.\n\n\nFile types like ZIP, PDF, Excel, etc. are crawlable if they are linked, but it may be useful to archive them if they represent a meaningful dataset, or if there are many of them on a page.\n\n\nThe crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as \na href ...\n tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.\n\n\nThe crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate \nthat\n).\n\n\nThe crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.\n\n\n\n\nYES\n\n\nIf the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:\n\n\n\n\nNominate it using the \nEDGI Nomination Chrome Extension\n.\n\n\nClick the \nDo not harvest\n checkbox in the Research section in the Archivers app.\n\n\nClick \nCheckin this URL\n and move on to another URL.\n\n\n\n\nNO\n\n\nIf it is confirmed not crawlable:\n\n\n\n\nSearch agency websites and data.gov for dataset entry points for your dataset collection.\n\n\nTips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.\n\n\nMake note of any better entry point in the \nRecommended Approach for Harvesting Data\n field, along with any other recommendations on how to proceed with this harvest.\n\n\n\n\n\n\nFill out all of the fields in the Research section to the best of your ability.\n\n\nOccasionally, URL's will have been nominated separately, but are actually different interfaces built on the same dataset. We want to scrape all of this data and do it exactly one time. The \nLink URL\n field lets you search for associated URLs; add any URLs that should be grouped into a single record.\n\n\n\n\nYES and NO\n\n\nFor example, FTP address, mixed content, big data sets:\n\n\n\n\n\nNominate it anyway, but also follow the steps for uncrawlable content above.\n\n\nWhile we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.\n\n\n\n\n\n  \nSee FAQ \n\"What is the difference between Crawlable and Harvested in Archivers.space?\"\n\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Research checkbox (far right on the same line as the \"Research\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Researching"
        }, 
        {
            "location": "/researching/#what-do-researchers-do", 
            "text": "Researchers review \"uncrawlables\" identified during  Seeding , confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.  \n   Recommended Skills     \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.", 
            "title": "What Do Researchers Do?"
        }, 
        {
            "location": "/researching/#getting-set-up-as-a-researcher", 
            "text": "Event organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.  They will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.    Create an account on the DataRefuge Slack using this  slack-in  or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions.  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event  Or post questions in the DataRefuge Slack  #researchers  channel (or other channel recommended by your event organizers).     \n   Researchers and Harvesters     \n   \n     Researchers and Harvesters should coordinate together as their work is closely related and benefits from close communication \n     It may be most effective to work together in pairs or small groups, or for a single person to both research and harvest \n     As a Researcher, make sure to check out the  Harvesting documentation  to familiarize yourself with their role", 
            "title": "Getting Set up as a Researcher"
        }, 
        {
            "location": "/researching/#claiming-a-dataset-to-research", 
            "text": "Using Archivers App     \n  Review our walkthrough video below and refer to the  FAQ  for any additional questions about the  Archivers app .  \n   \n      Researchers work on datasets that were listed as uncrawlable by Seeders.  Go to the  Archivers app , click  URLS  and then  RESEARCH : all the URLs listed are ready to be researched.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.  Priority is indicated by the \u201c!\u201d field.  The range is from 0 to 10, with 10 being highest priority.    Select an available URL (you may decide to select a URL relevant to your area of expertise or assigned a high priority) and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Dataset to Research"
        }, 
        {
            "location": "/researching/#evaluating-the-data", 
            "text": "Go to the URL and start inspecting the content.", 
            "title": "Evaluating the Data"
        }, 
        {
            "location": "/researching/#is-the-data-actually-crawlable", 
            "text": "Again, see  EDGI's Guides  for a mostly non-technical introduction to the crawler:   Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler   Some additional technical notes for answering this:   There is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.  File types like ZIP, PDF, Excel, etc. are crawlable if they are linked, but it may be useful to archive them if they represent a meaningful dataset, or if there are many of them on a page.  The crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as  a href ...  tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.  The crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate  that ).  The crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.", 
            "title": "Is the data actually crawlable?"
        }, 
        {
            "location": "/researching/#yes", 
            "text": "If the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:   Nominate it using the  EDGI Nomination Chrome Extension .  Click the  Do not harvest  checkbox in the Research section in the Archivers app.  Click  Checkin this URL  and move on to another URL.", 
            "title": "YES"
        }, 
        {
            "location": "/researching/#no", 
            "text": "If it is confirmed not crawlable:   Search agency websites and data.gov for dataset entry points for your dataset collection.  Tips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.  Make note of any better entry point in the  Recommended Approach for Harvesting Data  field, along with any other recommendations on how to proceed with this harvest.    Fill out all of the fields in the Research section to the best of your ability.  Occasionally, URL's will have been nominated separately, but are actually different interfaces built on the same dataset. We want to scrape all of this data and do it exactly one time. The  Link URL  field lets you search for associated URLs; add any URLs that should be grouped into a single record.", 
            "title": "NO"
        }, 
        {
            "location": "/researching/#yes-and-no", 
            "text": "For example, FTP address, mixed content, big data sets:   Nominate it anyway, but also follow the steps for uncrawlable content above.  While we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.   \n   See FAQ  \"What is the difference between Crawlable and Harvested in Archivers.space?\"", 
            "title": "YES and NO"
        }, 
        {
            "location": "/researching/#finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Research checkbox (far right on the same line as the \"Research\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/harvesting/", 
            "text": "What Do Harvesters Do?\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.\n\n\n\n\nGetting Set up as a Harvester\n\n\n\n\nThe organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through Slack or a form.\n\n\nThey will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.\n\n\nYou might also need other software and utilities set up on your computer, depending on the harvesting methods you use.\n\n\nHarvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on \nsemi-automated harvesting as part of a team\n, and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event\n\n\nOr post questions in the DataRefuge Slack \n#harvesters\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\n\n  \nResearchers and Harvesters\n \n  \n  \n\n    \nResearchers and Harvesters should coordinate together as their work is closely related and benefits from close communication\n\n    \nIt may be most effective to work together in pairs or small groups, or for a single person to both research and harvest\n\n    \nAs a Harvester, make sure to check out the \nResearching documentation\n to familiarize yourself with their role\n\n  \n\n\n\n\n\nHarvesting Tools\n\n\nFor in-depth information on tools and techniques to harvest open data, please check EDGI's extensive \nharvesting tools\n.\n\n\n1. Claiming a Dataset to Harvest\n\n\n\n  \nUsing Archivers App\n \n  \n  Review our walkthrough video below and refer to the \nFAQ\n for any additional questions about the \nArchivers app\n. \n\n  \n\n  \n\n\n\n\n\n\n\nYou will work on datasets that were confirmed as uncrawlable by Researchers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nHARVEST\n: all the URLs listed are ready to be harvested.\n\n\nAvailable URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nURL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\n2. Investigate the Dataset\n\n\n\n  \nA \"Meaningful Dataset\"\n \n  \n  Your role is to harvest datasets that are complete and \nmeaningful\n, by which we mean: \"will the dataset make sense to a scientist\"? \n\n  For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\n\n\n2a. Classify Source Type \n Archivability\n\n\nBefore doing anything, take a minute to understand what you're looking at. It's usually best to do a quick check of the URL to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the URL in question.\n\n\nCheck for False-Positives (Content That Is in Fact Crawlable)\n\n\nGenerally, any URL that returns standard HTML, links to more \nHTML mimetype pages\n, and contains little-to-no non-HTML content, is crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, nominate it to the Internet Archive using the \nEDGI Nomination Chrome Extension\n, click the \nDo not harvest\n checkbox in the Research section of the Archivers app, click \nCheckin this URL\n, and move on to another URL.\n\n\nSome Things to Think About While Reviewing a URL\n\n\n\n\nDoes this page use JavaScript to render its content, especially to \ngenerate links\n or \ndynamically pull up images and PDF content\n? Crawlers generally cannot parse dynamically generated content.\n\n\nDoes this URL contain links to non-HTML content? (For example, zip files, PDFs, Excel files, etc...)\n\n\nIs this URL some sort of interface for a large database or service? (For example, an interactive map, API gateway, etc.)\n\n\nDoes this URL contain instructions for connecting to a server, database, or other special source of data?\n\n\n\n\nCheck the Terms of Service!\n\n\nBefore you go any further, it is \nalways\n worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving, \nmake a note of it\n. Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically available source onto a site that has different terms of service.\n\n\nData acquired outside terms of service is not usable.\n\n\n2b. Determine Scale of the Dataset\n\n\nIf the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the \nEIS WARC archiver\n, which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue guide to determine how to best proceed.\n\n\n3. Generate HTML, JSON \n Directory\n\n\nTo get started, click \nDownload Zip Starter\n, which will download an empty zip archive structure for the data you are about to harvest.\nThe structure looks like this:\n\n\nDAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data\n\n\n\n\nEach row in the above is:\n\n\nA directory named by the UUID\n    \u251c\u2500\u2500 a .html \nweb archive\n file of the URL for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes \n files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question\n\n\n\n\nFolder Structure\n\n\nUUID\n\n\nThe goal is to pass this finalized folder off for \n\"bagging\"\n. We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied \nexactly\n wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.\n\n\n[UUID].html file\n\n\nThe zip starter archive will automatically include a copy of the page corresponding to the URL. The HTML file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the \n.html\n in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.\n\n\n[UUID].json file\n\n\nYou'll need to inspect the .json manifest to be sure all fields are correct. This file contains vital data, including the url that was archived and date of archiving. The manifest should contain the following fields:\n\n\n{\n    \nDate of capture\n: \n,\n    \nFile formats contained in package\n: \n,\n    \nFree text description of capture process\n: \n,\n    \nIndividual source or seed URL\n: \n,\n    \nInstitution facilitating the data capture creation and packaging\n: \n,\n    \nName of package creator\n: \n,\n    \nName of resource\n: \n,\n    \nType(s) of content in package\n: \n,\n    \nrecommended_approach\n: \n,\n    \nsignificance\n: \n,\n    \ntitle\n: \n,\n    \nurl\n: \n\n}\n\n\n\n\n[UUID]/tools/\n\n\nDirectory containing any scripts, notes \n files used to acquire the data. Put any scripts you write or tools you use into this directory. This is useful in case new data needs to be archived from the same site again at a later date.\n\n\n[UUID]/data/\n\n\nDirectory containing the data in question.\n\n\n4. Acquire the Data\n\n\nYour method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.\n\n\n4a. Identify Data Links \n Acquire Them in a wget Loop\n\n\nIf you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may work well. It's important to only use this approach when you encounter \ndata\n, for example PDF's, .zip archives, .csv datasets, etc.\n\n\nThe tricky part of this approach is generating a list of URLs to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful \nbeautiful-soup package\n), go for it. Otherwise, we've included the \njquery-url-extraction guide\n, which has the advantage of working within a browser and can operate on a page that has been modified by JavaScript.\n\n\nOur example dataset uses jquery-URL, \nleveraging that tool to generate a list of URLs to feed the wget loop\n.\n\n\n4b. Identify Data Links \n Acquire Them via WARCFactory\n\n\nFor search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the \nEIS WARC archiver\n or the \nEPA Search Utils\n for ideas on how to proceed.\n\n\n4c. FTP Download\n\n\nGovernment datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at \ndownload_ftp_tree.py\n as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated \nseparately\n from HTTP seeds).\n\n\n4d. API Scrape / Custom Solution\n\n\nIf you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.\n\n\n4e. Automated Full Browser\n\n\nThe last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as \nwget\n, \ncurl\n, or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a \nruby example\n.\n\n\nTips\n\n\n\n\nIf you encounter a search bar, try entering \"*\"\n to see if that returns \"all results\".\n\n\nLeave the data unmodified. During the process, you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.\n\n\n\n\n5. Complete [UUID].json \n Add /tools\n\n\nFrom there you'll want to complete the [UUID].json. Use the template below as a guide.\n\n\n\n\nThe json should match the information from the Researcher and use the following format:\n\n\n\n\n{\n    \nDate of capture\n: \nFri Feb 24 2017 21:44:07 GMT-0800 (PST)\n,\n    \nFile formats contained in package\n: \n.xls, .zip\n,\n    \nFree text description of capture process\n: \nMetadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged.\n,\n    \nIndividual source or seed URL\n: \nftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\n,\n    \nInstitution facilitating the data capture creation and packaging\n: \nDataRescue SF Bay\n,\n    \nName of package creator\n: \nJohnDoe\n,\n    \nName of resource\n: \nNimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\n,\n    \nType(s) of content in package\n: \nThese files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\n,\n    \nUUID\n: \nF3499E3B-7517-4C06-A661-72B4DA13A2A2\n,\n    \nrecommended_approach\n: \n,\n    \nsignificance\n: \nThese files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\n,\n    \ntitle\n: \nNimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\n,\n    \nurl\n: \nftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\n\n}\n\n\n\n\n\n\nMake sure to save this as a .json file.\n\n\n\n\nIn addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.\n\n\nIt's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.\n\n\n6. Uploading the data\n\n\n\n\nZip all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID.\n\n\nUpload the zip file by selecting \nChoose File\n and then clicking \nUpload\n in the Archivers app.\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide or post on Slack in the Harvesters channel, if you are having issues with this more advanced method.\n\n\n\n\n\n\n\n\n\n  \nSee FAQ \n\"What is the difference between Crawlable and Harvested in Archivers.space?\"\n\n\n\n\n\n7. Finishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Harvest checkbox (far right on the same line as the \"Harvest\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Harvesting"
        }, 
        {
            "location": "/harvesting/#what-do-harvesters-do", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.  \n   Recommended Skills     \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.", 
            "title": "What Do Harvesters Do?"
        }, 
        {
            "location": "/harvesting/#getting-set-up-as-a-harvester", 
            "text": "The organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through Slack or a form.  They will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.  You might also need other software and utilities set up on your computer, depending on the harvesting methods you use.  Harvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on  semi-automated harvesting as part of a team , and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event  Or post questions in the DataRefuge Slack  #harvesters  channel (or other channel recommended by your event organizers).     \n   Researchers and Harvesters     \n   \n     Researchers and Harvesters should coordinate together as their work is closely related and benefits from close communication \n     It may be most effective to work together in pairs or small groups, or for a single person to both research and harvest \n     As a Harvester, make sure to check out the  Researching documentation  to familiarize yourself with their role", 
            "title": "Getting Set up as a Harvester"
        }, 
        {
            "location": "/harvesting/#harvesting-tools", 
            "text": "For in-depth information on tools and techniques to harvest open data, please check EDGI's extensive  harvesting tools .", 
            "title": "Harvesting Tools"
        }, 
        {
            "location": "/harvesting/#1-claiming-a-dataset-to-harvest", 
            "text": "Using Archivers App     \n  Review our walkthrough video below and refer to the  FAQ  for any additional questions about the  Archivers app .  \n   \n      You will work on datasets that were confirmed as uncrawlable by Researchers.  Go to the  Archivers app , click  URLS  and then  HARVEST : all the URLs listed are ready to be harvested.  Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "1. Claiming a Dataset to Harvest"
        }, 
        {
            "location": "/harvesting/#2-investigate-the-dataset", 
            "text": "A \"Meaningful Dataset\"     \n  Your role is to harvest datasets that are complete and  meaningful , by which we mean: \"will the dataset make sense to a scientist\"?  \n  For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.", 
            "title": "2. Investigate the Dataset"
        }, 
        {
            "location": "/harvesting/#2a-classify-source-type-archivability", 
            "text": "Before doing anything, take a minute to understand what you're looking at. It's usually best to do a quick check of the URL to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the URL in question.", 
            "title": "2a. Classify Source Type &amp; Archivability"
        }, 
        {
            "location": "/harvesting/#check-for-false-positives-content-that-is-in-fact-crawlable", 
            "text": "Generally, any URL that returns standard HTML, links to more  HTML mimetype pages , and contains little-to-no non-HTML content, is crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, nominate it to the Internet Archive using the  EDGI Nomination Chrome Extension , click the  Do not harvest  checkbox in the Research section of the Archivers app, click  Checkin this URL , and move on to another URL.", 
            "title": "Check for False-Positives (Content That Is in Fact Crawlable)"
        }, 
        {
            "location": "/harvesting/#some-things-to-think-about-while-reviewing-a-url", 
            "text": "Does this page use JavaScript to render its content, especially to  generate links  or  dynamically pull up images and PDF content ? Crawlers generally cannot parse dynamically generated content.  Does this URL contain links to non-HTML content? (For example, zip files, PDFs, Excel files, etc...)  Is this URL some sort of interface for a large database or service? (For example, an interactive map, API gateway, etc.)  Does this URL contain instructions for connecting to a server, database, or other special source of data?", 
            "title": "Some Things to Think About While Reviewing a URL"
        }, 
        {
            "location": "/harvesting/#check-the-terms-of-service", 
            "text": "Before you go any further, it is  always  worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving,  make a note of it . Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically available source onto a site that has different terms of service.  Data acquired outside terms of service is not usable.", 
            "title": "Check the Terms of Service!"
        }, 
        {
            "location": "/harvesting/#2b-determine-scale-of-the-dataset", 
            "text": "If the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the  EIS WARC archiver , which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue guide to determine how to best proceed.", 
            "title": "2b. Determine Scale of the Dataset"
        }, 
        {
            "location": "/harvesting/#3-generate-html-json-directory", 
            "text": "To get started, click  Download Zip Starter , which will download an empty zip archive structure for the data you are about to harvest.\nThe structure looks like this:  DAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data  Each row in the above is:  A directory named by the UUID\n    \u251c\u2500\u2500 a .html  web archive  file of the URL for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes   files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question", 
            "title": "3. Generate HTML, JSON &amp; Directory"
        }, 
        {
            "location": "/harvesting/#folder-structure", 
            "text": "", 
            "title": "Folder Structure"
        }, 
        {
            "location": "/harvesting/#uuid", 
            "text": "The goal is to pass this finalized folder off for  \"bagging\" . We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied  exactly  wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.", 
            "title": "UUID"
        }, 
        {
            "location": "/harvesting/#uuidhtml-file", 
            "text": "The zip starter archive will automatically include a copy of the page corresponding to the URL. The HTML file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the  .html  in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.", 
            "title": "[UUID].html file"
        }, 
        {
            "location": "/harvesting/#uuidjson-file", 
            "text": "You'll need to inspect the .json manifest to be sure all fields are correct. This file contains vital data, including the url that was archived and date of archiving. The manifest should contain the following fields:  {\n     Date of capture :  ,\n     File formats contained in package :  ,\n     Free text description of capture process :  ,\n     Individual source or seed URL :  ,\n     Institution facilitating the data capture creation and packaging :  ,\n     Name of package creator :  ,\n     Name of resource :  ,\n     Type(s) of content in package :  ,\n     recommended_approach :  ,\n     significance :  ,\n     title :  ,\n     url :  \n}", 
            "title": "[UUID].json file"
        }, 
        {
            "location": "/harvesting/#uuidtools", 
            "text": "Directory containing any scripts, notes   files used to acquire the data. Put any scripts you write or tools you use into this directory. This is useful in case new data needs to be archived from the same site again at a later date.", 
            "title": "[UUID]/tools/"
        }, 
        {
            "location": "/harvesting/#uuiddata", 
            "text": "Directory containing the data in question.", 
            "title": "[UUID]/data/"
        }, 
        {
            "location": "/harvesting/#4-acquire-the-data", 
            "text": "Your method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.", 
            "title": "4. Acquire the Data"
        }, 
        {
            "location": "/harvesting/#4a-identify-data-links-acquire-them-in-a-wget-loop", 
            "text": "If you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may work well. It's important to only use this approach when you encounter  data , for example PDF's, .zip archives, .csv datasets, etc.  The tricky part of this approach is generating a list of URLs to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful  beautiful-soup package ), go for it. Otherwise, we've included the  jquery-url-extraction guide , which has the advantage of working within a browser and can operate on a page that has been modified by JavaScript.  Our example dataset uses jquery-URL,  leveraging that tool to generate a list of URLs to feed the wget loop .", 
            "title": "4a. Identify Data Links &amp; Acquire Them in a wget Loop"
        }, 
        {
            "location": "/harvesting/#4b-identify-data-links-acquire-them-via-warcfactory", 
            "text": "For search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the  EIS WARC archiver  or the  EPA Search Utils  for ideas on how to proceed.", 
            "title": "4b. Identify Data Links &amp; Acquire Them via WARCFactory"
        }, 
        {
            "location": "/harvesting/#4c-ftp-download", 
            "text": "Government datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at  download_ftp_tree.py  as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated  separately  from HTTP seeds).", 
            "title": "4c. FTP Download"
        }, 
        {
            "location": "/harvesting/#4d-api-scrape-custom-solution", 
            "text": "If you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.", 
            "title": "4d. API Scrape / Custom Solution"
        }, 
        {
            "location": "/harvesting/#4e-automated-full-browser", 
            "text": "The last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as  wget ,  curl , or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a  ruby example .", 
            "title": "4e. Automated Full Browser"
        }, 
        {
            "location": "/harvesting/#tips", 
            "text": "If you encounter a search bar, try entering \"*\"  to see if that returns \"all results\".  Leave the data unmodified. During the process, you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.", 
            "title": "Tips"
        }, 
        {
            "location": "/harvesting/#5-complete-uuidjson-add-tools", 
            "text": "From there you'll want to complete the [UUID].json. Use the template below as a guide.   The json should match the information from the Researcher and use the following format:   {\n     Date of capture :  Fri Feb 24 2017 21:44:07 GMT-0800 (PST) ,\n     File formats contained in package :  .xls, .zip ,\n     Free text description of capture process :  Metadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged. ,\n     Individual source or seed URL :  ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7 ,\n     Institution facilitating the data capture creation and packaging :  DataRescue SF Bay ,\n     Name of package creator :  JohnDoe ,\n     Name of resource :  Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984 ,\n     Type(s) of content in package :  These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc. ,\n     UUID :  F3499E3B-7517-4C06-A661-72B4DA13A2A2 ,\n     recommended_approach :  ,\n     significance :  These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc. ,\n     title :  Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984 ,\n     url :  ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7 \n}   Make sure to save this as a .json file.   In addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.  It's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.", 
            "title": "5. Complete [UUID].json &amp; Add /tools"
        }, 
        {
            "location": "/harvesting/#6-uploading-the-data", 
            "text": "Zip all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID.  Upload the zip file by selecting  Choose File  and then clicking  Upload  in the Archivers app.  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide or post on Slack in the Harvesters channel, if you are having issues with this more advanced method.     \n   See FAQ  \"What is the difference between Crawlable and Harvested in Archivers.space?\"", 
            "title": "6. Uploading the data"
        }, 
        {
            "location": "/harvesting/#7-finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Harvest checkbox (far right on the same line as the \"Harvest\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "7. Finishing up"
        }, 
        {
            "location": "/bagging/", 
            "text": "What Do Baggers Do?\n\n\nBaggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\nNote: Checking is currently performed by Baggers and does not exist as a separate stage in the Archivers app.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.\n\n\n\n\nGetting Set up as a Bagger\n\n\n\n\nApply to become a Bagger by filling out \nthis form\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\nVerify that you have bagging permissions by going to the \nArchivers app\n, clicking \nURLS\n, and confirming that you can see a section called \"Bag\".\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   \n\n\nGet set up with Python and the \nbagit-python\n script to make a bag at the command line\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event.\n\n\nOr post questions in the DataRefuge Slack \n#baggers\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nClaiming a Dataset for Bagging\n\n\n\n\nYou will work on datasets that were harvested by Harvesters.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nBAG\n: all the URLs listed are ready to be bagged.\n\n\nAvailable URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nDownloading \n Opening the Dataset\n\n\n\n\nThe zipped dataset that is ready to be bagged is under \nHarvest Url / Location\n in the the Archivers app. Download it to your laptop and unzip it.\n\n\nExtra check: Is this URL truly ready to bag?\n\n\nWhile everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:\n\n\nThere is no value in the \"Harvest Url / Location\" field.\n\n\nIf you don't see a \"Harvest Url / Location\" field at all, confirm that you have bagging privileges.\n\n\n_Please note that even though the Archivers app field is not populated, in some case you might still be able to locate the file on our cloud storage. Check for the file presence by using the following URL structure: \nhttps://drp-upload.s3.amazonaws.com/remote/ + [UUID] + .zip\n, so for instance: \nhttps://drp-upload.s3.amazonaws.com/remote/13E0A60E-2324-4321-927D-8496F136B2B5.zip\n\n\n\n\n\n\nThere is a note in the Harvest section that seems to indicate that the harvest was only partially performed.  \n\n\nIn either case, uncheck the \"Harvest\" checkbox, and add a note in the \nNotes From Harvest\n field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Assurance\n\n\n\n\nConfirm the harvested files:\n\n\nGo to the original URL and check that the dataset is complete and accurate.\n\n\nYou also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\nSpot-check to make sure the files open properly and are not faulty in any way.\n\n\n\n\n\n\nConfirm contents of JSON file:\n\n\nThe JSON should match the information from the Researcher and use the following format:  \n\n\n\n\n\n\n\n\n{\n    \nDate of capture\n: \nFri Feb 24 2017 21:44:07 GMT-0800 (PST)\n,\n    \nFile formats contained in package\n: \n.xls, .zip\n,\n    \nFree text description of capture process\n: \nMetadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged.\n,\n    \nIndividual source or seed URL\n: \nftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\n,\n    \nInstitution facilitating the data capture creation and packaging\n: \nDataRescue SF Bay\n,\n    \nName of package creator\n: \nJohnDoe\n,\n    \nName of resource\n: \nNimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\n,\n    \nType(s) of content in package\n: \nThese files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\n,\n    \nUUID\n: \nF3499E3B-7517-4C06-A661-72B4DA13A2A2\n,\n    \nrecommended_approach\n: \n,\n    \nsignificance\n: \nThese files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\n,\n    \ntitle\n: \nNimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\n,\n    \nurl\n: \nftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\n\n}\n\n\n\n\n\n\nIf you make any changes, make sure to save this as a .json file.\n\n\nConfirm that the JSON file is within the package with the dataset(s).\n\n\n\n\nCreating the Bag\n\n\n\n\nRun the python command line script that creates the bag:\n\n\n\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\n\n\n\n\nYou should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files:\n\n\nbag-info.txt\n\n\nbagit.txt\n\n\nmanifest-md5.txt\n\n\ntagmanifest-md5.txt\n\n\n\n\n\n\nIMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.\n\n\nRun the following python command line script to do an initial validation of a bag:\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\n\n\nIf it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n\n\n\n\nCreating the Zip File and Uploading It\n\n\n\n\nZip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID.\n\n\nWithout moving the file\n, upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided in the Archivers App\n\n\nMake sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)\n\n\nThe application will return the location URL for your zip file.\n\n\nThe syntax will be \n[UrlStub]/[UUID].zip\n\n\nCopy and paste that URL to the \nBag URL\n field in the Archivers app.\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide or post on Slack in the Baggers channel, if you are having issues with this more advanced method.\n\n\n\n\nQuality Assurance and Finishing Up\n\n\n\n\nOnce the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation:\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\n\n\nIf it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n\n\nFill out as much information as possible in the \nNotes From Bagging\n field in the Archivers app to document your work.\n\n\nCheck the checkbox that certifies this is a \"well-checked bag\".\n\n\nCheck the Bag checkbox (far right on the same line as the \"Bag\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n to release it and allow someone else to work on the next step.", 
            "title": "Checking/Bagging"
        }, 
        {
            "location": "/bagging/#what-do-baggers-do", 
            "text": "Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.  Note: Checking is currently performed by Baggers and does not exist as a separate stage in the Archivers app.  \n   Recommended Skills     \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.", 
            "title": "What Do Baggers Do?"
        }, 
        {
            "location": "/bagging/#getting-set-up-as-a-bagger", 
            "text": "Apply to become a Bagger by filling out  this form  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.  Verify that you have bagging permissions by going to the  Archivers app , clicking  URLS , and confirming that you can see a section called \"Bag\".    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.     Get set up with Python and the  bagit-python  script to make a bag at the command line  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event.  Or post questions in the DataRefuge Slack  #baggers  channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Bagger"
        }, 
        {
            "location": "/bagging/#claiming-a-dataset-for-bagging", 
            "text": "You will work on datasets that were harvested by Harvesters.  Go to the  Archivers app , click  URLS  and then  BAG : all the URLs listed are ready to be bagged.  Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Dataset for Bagging"
        }, 
        {
            "location": "/bagging/#downloading-opening-the-dataset", 
            "text": "The zipped dataset that is ready to be bagged is under  Harvest Url / Location  in the the Archivers app. Download it to your laptop and unzip it.  Extra check: Is this URL truly ready to bag?  While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:  There is no value in the \"Harvest Url / Location\" field.  If you don't see a \"Harvest Url / Location\" field at all, confirm that you have bagging privileges.  _Please note that even though the Archivers app field is not populated, in some case you might still be able to locate the file on our cloud storage. Check for the file presence by using the following URL structure:  https://drp-upload.s3.amazonaws.com/remote/ + [UUID] + .zip , so for instance:  https://drp-upload.s3.amazonaws.com/remote/13E0A60E-2324-4321-927D-8496F136B2B5.zip    There is a note in the Harvest section that seems to indicate that the harvest was only partially performed.    In either case, uncheck the \"Harvest\" checkbox, and add a note in the  Notes From Harvest  field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.", 
            "title": "Downloading &amp; Opening the Dataset"
        }, 
        {
            "location": "/bagging/#quality-assurance", 
            "text": "Confirm the harvested files:  Go to the original URL and check that the dataset is complete and accurate.  You also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.  Spot-check to make sure the files open properly and are not faulty in any way.    Confirm contents of JSON file:  The JSON should match the information from the Researcher and use the following format:       {\n     Date of capture :  Fri Feb 24 2017 21:44:07 GMT-0800 (PST) ,\n     File formats contained in package :  .xls, .zip ,\n     Free text description of capture process :  Metadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged. ,\n     Individual source or seed URL :  ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7 ,\n     Institution facilitating the data capture creation and packaging :  DataRescue SF Bay ,\n     Name of package creator :  JohnDoe ,\n     Name of resource :  Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984 ,\n     Type(s) of content in package :  These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc. ,\n     UUID :  F3499E3B-7517-4C06-A661-72B4DA13A2A2 ,\n     recommended_approach :  ,\n     significance :  These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc. ,\n     title :  Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984 ,\n     url :  ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7 \n}   If you make any changes, make sure to save this as a .json file.  Confirm that the JSON file is within the package with the dataset(s).", 
            "title": "Quality Assurance"
        }, 
        {
            "location": "/bagging/#creating-the-bag", 
            "text": "Run the python command line script that creates the bag:   bagit.py --contact-name '[your name]' /directory/to/bag   You should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files:  bag-info.txt  bagit.txt  manifest-md5.txt  tagmanifest-md5.txt    IMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.  Run the following python command line script to do an initial validation of a bag:   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.", 
            "title": "Creating the Bag"
        }, 
        {
            "location": "/bagging/#creating-the-zip-file-and-uploading-it", 
            "text": "Zip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID.  Without moving the file , upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided in the Archivers App  Make sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)  The application will return the location URL for your zip file.  The syntax will be  [UrlStub]/[UUID].zip  Copy and paste that URL to the  Bag URL  field in the Archivers app.  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide or post on Slack in the Baggers channel, if you are having issues with this more advanced method.", 
            "title": "Creating the Zip File and Uploading It"
        }, 
        {
            "location": "/bagging/#quality-assurance-and-finishing-up", 
            "text": "Once the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation:   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.  Fill out as much information as possible in the  Notes From Bagging  field in the Archivers app to document your work.  Check the checkbox that certifies this is a \"well-checked bag\".  Check the Bag checkbox (far right on the same line as the \"Bag\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL  to release it and allow someone else to work on the next step.", 
            "title": "Quality Assurance and Finishing Up"
        }, 
        {
            "location": "/describing/", 
            "text": "What Do Describers Do?\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with metadata practices.\n\n\n\n\nGetting Set up as a Describer\n\n\n\n\nApply to become a Describer by asking your DataRescue guide or by filling out \nthis form\n.\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for DataRefuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite.\n\n\n\n\n\n\nThe organizers will also create an account for you in the \ndatarefuge.org\n CKAN instance.\n\n\nTest that you can log in successfully.\n\n\n\n\n\n\nGet set up with Python and the \nbagit-python\n script to make a bag at the command line\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event.\n\n\nOr post questions in the DataRefuge Slack \n#describers\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nClaiming a Bag\n\n\n\n\nYou will work on datasets that were bagged by Baggers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nDESCRIBE\n: all the URLs listed are ready to be added to the CKAN instance.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nQA Step\n\n\n\n\nIn the Archivers app, scroll down to the \nDescribe\n section.\n\n\nThe URL of the zipped bag is in the \nBag Url / Location\n field.\n\n\nCut and paste that URL into your browser and download it.\n\n\nAfter downloading, unzip it.\n\n\nSpot-check some of the files (make sure they open and look normal, i.e., not garbled).\n\n\nIf the file fails QA:\n\n\nUncheck the Bagging checkbox.\n\n\nMake a note in the \nNotes From Bagging\n field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.\n\n\n\n\n\n\n\n\nCreate New Record in CKAN\n\n\n\n\nGo to \nCKAN\n and click Organizations in the top menu.\n\n\nChoose the organization (i.e., federal agency) that your dataset belongs to, e.g. \nNOAA\n, and click it.\n\n\nIf the Organization you need does not exist yet, create it by clicking \nAdd Organization\n.\n\n\n\n\n\n\nClick \"Add Dataset\".\n\n\nStart entering metadata in the new record, following the metadata template below:\n\n\nTitle:\n Title of dataset, e.g., \"Form EIA-411 Data\".\n\n\nCustom Text: DO NOT Fill OUT (this field does not function properly at this time)\n\n\nDescription:\n Usually copied and pasted description found on webpage.\n\n\nTags:\n Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".\n\n\nLicense:\n Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".\n\n\nOrganization:\n Choose value in dropdown, e.g., \"United States Department of Energy\".\n\n\nVisibility:\n Select \"Public\".\n\n\nSource:\n URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".\n\n\n\n\n\n\nTo decide what value to enter in each field:\n\n\nOpen the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.\n\n\nGo to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.\n\n\nAlternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Existing Metadata\n\n\nThese sites have federally-sourced metadata that can be added to the CKAN record for more accurate metadata:\n\n\n\n\nEPA:\n\n\nhttps://www.epa.gov/enviro/facility-registry-service-frs\n\n\nhttps://edg.epa.gov/metadata/catalog/main/home.page\n\n\n\n\n\n\n\n\nThese sites are sources of scientific metadata standards to review when choosing keywords:\n\n\n\n\nGCMD Keywords, downloadable CSV files of the GCMD taxonomies:\n\n\nhttps://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access\n\n\n\n\n\n\nATRAC, a free tool for accessing geographic metadata standards including auto-populating thesauri (GCMD and others commonly used with climate data):\n\n\nhttps://www.ncdc.noaa.gov/atrac/index.html\n\n\n\n\n\n\n\n\nLinking the CKAN Record to the Bag\n\n\n\n\nClick \"Next: Add Data\" at the bottom of the CKAN form.\n\n\nEnter the following information:\n\n\nLink:\n Bag URL, e.g., \nhttps://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nName:\n filename, e.g., \n77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nFormat:\n select \"Zip\".\n\n\n\n\n\n\nClick \"Finish\".\n\n\nTest that the link you just created works by clicking it, and verifying that the file begins to download.\n\n\nNote that you don't need to finish downloading it again.\n\n\nAlternatively, use WGET to test without downloading: \nwget --spider [BAG URL]\n\n\n\n\n\n\n\n\nAdding the CKAN record to the \"Data Rescue Events\" group\n\n\n\n\nOnce the record is created, click the tab \nGroups\n  \n\n\nSelect \nData Rescue Events\n in the dropdown and click \nAdd to Group\n.\n\n\nIn the future, it will be useful to be able to differentiate that among different groups of records based on how they were generated.\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, add the URL to the CKAN record in the \nCKAN URL\n field.\n\n\nThe syntax will be:\n \nhttps://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]\n\n\n\n\n\n\nAdd any useful notes to document your work.\n\n\nCheck the Describe checkbox (far right on the same line as the \"Describe\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n, to release it.\n\n\n\n\nPossible Tools: JSON Viewers\n\n\n\n\njsoneditoronline.org\n\n\njsonviewer.stack.hu", 
            "title": "Describing"
        }, 
        {
            "location": "/describing/#what-do-describers-do", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.  \n   Recommended Skills     \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with metadata practices.", 
            "title": "What Do Describers Do?"
        }, 
        {
            "location": "/describing/#getting-set-up-as-a-describer", 
            "text": "Apply to become a Describer by asking your DataRescue guide or by filling out  this form .  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for DataRefuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite.    The organizers will also create an account for you in the  datarefuge.org  CKAN instance.  Test that you can log in successfully.    Get set up with Python and the  bagit-python  script to make a bag at the command line  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event.  Or post questions in the DataRefuge Slack  #describers  channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Describer"
        }, 
        {
            "location": "/describing/#claiming-a-bag", 
            "text": "You will work on datasets that were bagged by Baggers.  Go to the  Archivers app , click  URLS  and then  DESCRIBE : all the URLs listed are ready to be added to the CKAN instance.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Bag"
        }, 
        {
            "location": "/describing/#qa-step", 
            "text": "In the Archivers app, scroll down to the  Describe  section.  The URL of the zipped bag is in the  Bag Url / Location  field.  Cut and paste that URL into your browser and download it.  After downloading, unzip it.  Spot-check some of the files (make sure they open and look normal, i.e., not garbled).  If the file fails QA:  Uncheck the Bagging checkbox.  Make a note in the  Notes From Bagging  field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.", 
            "title": "QA Step"
        }, 
        {
            "location": "/describing/#create-new-record-in-ckan", 
            "text": "Go to  CKAN  and click Organizations in the top menu.  Choose the organization (i.e., federal agency) that your dataset belongs to, e.g.  NOAA , and click it.  If the Organization you need does not exist yet, create it by clicking  Add Organization .    Click \"Add Dataset\".  Start entering metadata in the new record, following the metadata template below:  Title:  Title of dataset, e.g., \"Form EIA-411 Data\".  Custom Text: DO NOT Fill OUT (this field does not function properly at this time)  Description:  Usually copied and pasted description found on webpage.  Tags:  Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".  License:  Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".  Organization:  Choose value in dropdown, e.g., \"United States Department of Energy\".  Visibility:  Select \"Public\".  Source:  URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".    To decide what value to enter in each field:  Open the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.  Go to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.  Alternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.", 
            "title": "Create New Record in CKAN"
        }, 
        {
            "location": "/describing/#enhancing-existing-metadata", 
            "text": "These sites have federally-sourced metadata that can be added to the CKAN record for more accurate metadata:   EPA:  https://www.epa.gov/enviro/facility-registry-service-frs  https://edg.epa.gov/metadata/catalog/main/home.page     These sites are sources of scientific metadata standards to review when choosing keywords:   GCMD Keywords, downloadable CSV files of the GCMD taxonomies:  https://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access    ATRAC, a free tool for accessing geographic metadata standards including auto-populating thesauri (GCMD and others commonly used with climate data):  https://www.ncdc.noaa.gov/atrac/index.html", 
            "title": "Enhancing Existing Metadata"
        }, 
        {
            "location": "/describing/#linking-the-ckan-record-to-the-bag", 
            "text": "Click \"Next: Add Data\" at the bottom of the CKAN form.  Enter the following information:  Link:  Bag URL, e.g.,  https://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Name:  filename, e.g.,  77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Format:  select \"Zip\".    Click \"Finish\".  Test that the link you just created works by clicking it, and verifying that the file begins to download.  Note that you don't need to finish downloading it again.  Alternatively, use WGET to test without downloading:  wget --spider [BAG URL]", 
            "title": "Linking the CKAN Record to the Bag"
        }, 
        {
            "location": "/describing/#adding-the-ckan-record-to-the-data-rescue-events-group", 
            "text": "Once the record is created, click the tab  Groups     Select  Data Rescue Events  in the dropdown and click  Add to Group .  In the future, it will be useful to be able to differentiate that among different groups of records based on how they were generated.", 
            "title": "Adding the CKAN record to the \"Data Rescue Events\" group"
        }, 
        {
            "location": "/describing/#finishing-up", 
            "text": "In the Archivers app, add the URL to the CKAN record in the  CKAN URL  field.  The syntax will be:\n  https://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]    Add any useful notes to document your work.  Check the Describe checkbox (far right on the same line as the \"Describe\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL , to release it.", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/describing/#possible-tools-json-viewers", 
            "text": "jsoneditoronline.org  jsonviewer.stack.hu", 
            "title": "Possible Tools: JSON Viewers"
        }, 
        {
            "location": "/organizing/pre-event/", 
            "text": "Below we've outlined the critical technical considerations for planning a DataRescue event.\n\n\nKey Steps\n\n\n\n\nRead the \nDataRescue Paths\n available as part of \nDataRefuge's Overview\n or \nEDGI's DataRescue Event Toolkit\n.\n\n\nJoin the \nDataRefuge Slack team\n and start a channel for your event.\n\n\nReview the \nworkflow documentation\n and decide which paths your event will have.\n\n\nSchedule a call with DataRefuge\n to:\n\n\nreview the workflow and confirm event logistics like volunteer support\n\n\nreceive access to the \nArchivers app\n to archive complex datasets\n\n\n\n\n\n\nSchedule a call with EDGI\n to:\n\n\nreceive training on using \nAgency Primers\n and EDGI's \nChrome Extension\n to identify and preserve web pages on federal government web sites\n\n\nreceive an orientation on event \nharvesting tools\n\n\n\n\n\n\n\n\nEvent Preservation Tools\n\n\nArchivers App\n\n\nA DataRefuge organizer will set up your event in the app and coordinate initial account creation. The \nArchivers app\n enables us to keep track of all the DataRescue event preservation and coordinate the work across different roles.\n\n\nThe app includes URLs coming from two main sources:\n- URLs nominated by Seeders at previous DataRescue events\n- URLs identified by a Union of Concerned Scientists survey which asked the scientific community to list the most vulnerable and important data currently accessible through federal websites.\n\n\nAgency Primers and Chrome Extension for Seeding\n\n\nAn EDGI coordinator will set up access to Agency Primer and Sub-primer documents as well as a seed progress spreadsheet. These documents will inform the work of the Seeders at your event. They will tell them which website or website sections they should be focusing on for URL discovery.\n\n\n\n  \nCrawl vs. Harvest: Where is the Data Stored?\n \n  \n  The workflow is designed to triage whether a URL will be stored by the Internet Archive or in the \nDataRefuge repository\n based on whether it can be automatically crawled by the Internet Archive web crawler or needs to be manually harvested.\n\n  \n\n    \nNominating crawlable URLs makes use of Internet Archive's existing infrastructure. See \nSeeding\n for more information on this process.\n\n    \nDatasets manually harvested are uploaded through the Archivers app to an Amazon S3 storage managed by DataRefuge.\n\n  \n\n\n\n\n\nPermissions and Credentials\n\n\n\n\nAll\n Path II Attendees need to have an account on the \nArchivers app\n.\n\n\nYou will need to generate invites for each one \nwithin the app\n, and paste the URL generated in a Slack Direct Message or email.\n\n\nEach participant invited will automatically \"belong\" to your event in the app.\n\n\n\n\n\n\nIn addition, Checkers and Baggers need to be given additional privileges in the app to access the Checking (i.e. \"Finalize\") and Bagging sections.\n\n\n\n\nTechnical Resources\n\n\n\n\nAccess to Wi-Fi\n\n\nExtra Power Strips and Extension Cords\n\n\nBackup storage (e.g., large (\n16GB) thumb drives)\n\n\nBackup cloud compute resources", 
            "title": "Before an Event"
        }, 
        {
            "location": "/organizing/pre-event/#key-steps", 
            "text": "Read the  DataRescue Paths  available as part of  DataRefuge's Overview  or  EDGI's DataRescue Event Toolkit .  Join the  DataRefuge Slack team  and start a channel for your event.  Review the  workflow documentation  and decide which paths your event will have.  Schedule a call with DataRefuge  to:  review the workflow and confirm event logistics like volunteer support  receive access to the  Archivers app  to archive complex datasets    Schedule a call with EDGI  to:  receive training on using  Agency Primers  and EDGI's  Chrome Extension  to identify and preserve web pages on federal government web sites  receive an orientation on event  harvesting tools", 
            "title": "Key Steps"
        }, 
        {
            "location": "/organizing/pre-event/#event-preservation-tools", 
            "text": "", 
            "title": "Event Preservation Tools"
        }, 
        {
            "location": "/organizing/pre-event/#archivers-app", 
            "text": "A DataRefuge organizer will set up your event in the app and coordinate initial account creation. The  Archivers app  enables us to keep track of all the DataRescue event preservation and coordinate the work across different roles.  The app includes URLs coming from two main sources:\n- URLs nominated by Seeders at previous DataRescue events\n- URLs identified by a Union of Concerned Scientists survey which asked the scientific community to list the most vulnerable and important data currently accessible through federal websites.", 
            "title": "Archivers App"
        }, 
        {
            "location": "/organizing/pre-event/#agency-primers-and-chrome-extension-for-seeding", 
            "text": "An EDGI coordinator will set up access to Agency Primer and Sub-primer documents as well as a seed progress spreadsheet. These documents will inform the work of the Seeders at your event. They will tell them which website or website sections they should be focusing on for URL discovery.  \n   Crawl vs. Harvest: Where is the Data Stored?     \n  The workflow is designed to triage whether a URL will be stored by the Internet Archive or in the  DataRefuge repository  based on whether it can be automatically crawled by the Internet Archive web crawler or needs to be manually harvested. \n   \n     Nominating crawlable URLs makes use of Internet Archive's existing infrastructure. See  Seeding  for more information on this process. \n     Datasets manually harvested are uploaded through the Archivers app to an Amazon S3 storage managed by DataRefuge.", 
            "title": "Agency Primers and Chrome Extension for Seeding"
        }, 
        {
            "location": "/organizing/pre-event/#permissions-and-credentials", 
            "text": "All  Path II Attendees need to have an account on the  Archivers app .  You will need to generate invites for each one  within the app , and paste the URL generated in a Slack Direct Message or email.  Each participant invited will automatically \"belong\" to your event in the app.    In addition, Checkers and Baggers need to be given additional privileges in the app to access the Checking (i.e. \"Finalize\") and Bagging sections.", 
            "title": "Permissions and Credentials"
        }, 
        {
            "location": "/organizing/pre-event/#technical-resources", 
            "text": "Access to Wi-Fi  Extra Power Strips and Extension Cords  Backup storage (e.g., large ( 16GB) thumb drives)  Backup cloud compute resources", 
            "title": "Technical Resources"
        }, 
        {
            "location": "/organizing/post-event/", 
            "text": "Key Steps\n\n\n\n\nSchedule a debrief call\n with EDGI regarding the Seeding status using the \nAgency Primers\n\n\nFollow up about the final disposition of the data\n with \nDataRefuge\n and \nEDGI\n\n\nAre there large datasets that need to be uploaded to S3 by an authorized person?\n\n\nHas someone taken responsibility for handing seeds off to the Internet Archive?\n\n\n\n\n\n\nProvide feedback about the event\n processes to other DataRescue groups\n\n\n\n\nOngoing Involvement\n\n\nDataRescue is a coalition and a movement. When the event is over, and exhaustion is setting in over a couple of rounds... there is still work to do.\n\n\n\n\nParticipants might want to continue the work started at the event, this is possible as our workflow is meant to function in-person as well as remotely.\n\n\nThere are emerging opportunities posted in the \nDataRefuge\n and \nArchivers\n Slack teams about how to be involved in the data protection movement to contribute your ideas, energy, and good will to building a sustainable future for knowledge.", 
            "title": "After an Event"
        }, 
        {
            "location": "/organizing/post-event/#key-steps", 
            "text": "Schedule a debrief call  with EDGI regarding the Seeding status using the  Agency Primers  Follow up about the final disposition of the data  with  DataRefuge  and  EDGI  Are there large datasets that need to be uploaded to S3 by an authorized person?  Has someone taken responsibility for handing seeds off to the Internet Archive?    Provide feedback about the event  processes to other DataRescue groups", 
            "title": "Key Steps"
        }, 
        {
            "location": "/organizing/post-event/#ongoing-involvement", 
            "text": "DataRescue is a coalition and a movement. When the event is over, and exhaustion is setting in over a couple of rounds... there is still work to do.   Participants might want to continue the work started at the event, this is possible as our workflow is meant to function in-person as well as remotely.  There are emerging opportunities posted in the  DataRefuge  and  Archivers  Slack teams about how to be involved in the data protection movement to contribute your ideas, energy, and good will to building a sustainable future for knowledge.", 
            "title": "Ongoing Involvement"
        }, 
        {
            "location": "/faq/", 
            "text": "The \nArchivers.space\n application is extremely fresh, we have some known issues and workarounds documented below.\n\n\n1) I'm looking at a URL, but I can't edit anything!\n\n\nMake sure you have clicked the big blue button \"\nCheckout this URL\n\" near the top. None of the fields can be edited until the URL is checked out.\n\n\n2) Why are URLs that may have already been archived by Ann Arbor available to research?\n\n\nWhen selecting a URL to review \"\n0\n\" is the \ndefault\n priority; it generally means that no-one has reviewed it \nUNLESS\n it says \nMAY HAVE BEEN HARVESTED AT ANN ARBOR\n.\n\n\nIn those cases, assign the priority to \"\n1\n\", so the URL drops down in the queue and then \nSKIP IT\n.\n\n\n3) What does it mean if it says \nCrawled by Internet Archive\n: Yes?\n\n\n\"\nCrawled by Internet Archive\n\" means the \npage itself\n was crawled; it may or may not mean the \ndataset\n was crawled.\n\n\nBased on what Heretrix \nCan and Can't Crawl\n, you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as \nDo not harvest\n.\n\n\n4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?\n\n\nIn cases like \nnoaa.gov\n, you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.\n\n\nComplete the \nResearch\n section to the best of your abilities, especially the \nRecommended Approach for Harvesting Data\n.\n\n\n5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?\n\n\nNot yet; addressing these datasets is a goal going-forward.\n\n\nCurrently, indicate in the notes in both the \nResearch\n and \nHarvest\n sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).\n\n\n6) What if I have a site and want to know if it has been crawled already?\n\n\nInternet Archive has both a \nWayback Machine Chrome Extension\n and \nAPIs\n you can use to check if something has been archived:\n\n\n\n\nWayback Machine Chrome Extension\n\n\nYou can also check on the Internet Archive site directly at \narchive.org/web/\n\n\nWayback CDX Server API\n\n\n\n\nThere is a \ncheck-ia\n script in the \nharvesting tools\n for batch URLs.\n\n\n7) What if the site has in fact been crawled well by the Internet Archive?\n\n\nIf the site includes only crawlable data, then there is no need to harvest it. These should be marked \nDo not harvest\n in the \nResearch\n phase.\n\n\nIf the site includes one of the forms of uncrawlable content:\n\n1) FTP\n\n2) Many Files\n\n3) Database\n\n4) Visualization/Interactive  \n\n\nThen mark accordingly in and harvest the datasets.\n\n\n8) What does it mean when it says \"checking out this url will unlock url: xxx\"?\n\n\nThat means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.\n\n\n9) What do I do when there is stuff listed in the Link URL section?\n\n\nIf there are a bunch of sub-sites listed that are \nnot\n links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.\n\n\nIf the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.\n\n\n10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download \n 5 GB)?\n\n\nFrom the \noverview pane\n, click \nAdd Url\n on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.\n\n\nCheck out the parent URL, and under \nResearch\n use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.\n\n\n11) Wifi is kind of Slow, are there workarounds for a faster connection?\n\n\n\n\n\n\nDo as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something, \nssh\n to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.\n\n\n\n\n\n\nTether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!\n\n\n\n\n\n\n12) Why can't I edit the harvesting section?\n\n\nArchivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the \nHarvesting\n section, you will first need to mark \nResearch\n as complete. Look for the checkbox on the right-hand side at the top of the \nResearch\n section. Once you've checked it, make sure to hit \nSave\n.\n\n\n13) When harvesting, why doesn't clicking on the \nDownload Zipstarter\n button work?\n\n\nUnfortunately this is a known issue. Make sure you've marked \nResearch\n complete. Try reloading the page, or switching browsers if you can.\n\n\nThe App is not compatible with Safari.\n\n\n14) In the \nResearch\n section, what are all the checkboxes for?\n\n\nPlease read the DataRescue Workflow documentation for more info!\n\n\n15) I have a process improvement that would make this go better!\n\n\nGreat! Open an issue in the \narchivers.space GitHub repository\n, or report it in the appropriate channel in your Slack team.\n\n\n16) How do I add a new event?\n\n\nAdmins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!\n\n\n17) What is the difference between Crawlable and Harvested?\n\n\nResearchers\n investigate whether URLs listed in the \nArchivers app\n need to be manually downloaded (harvested) or if they can be automatically saved (crawled) by the Internet Archive. The URLs listed as \nCrawlable\n were determined as such during that research phase and are submitted to the Internet Archive, they do not need to be harvested. \n\n\nThese URLs represent only a small portion of those submitted to the Internet Archive from DataRescue events. Most crawlable URLs are identified by \nSeeders\n at the beginning of the workflow and completely bypass the Archivers app.", 
            "title": "Archivers App FAQ"
        }, 
        {
            "location": "/faq/#1-im-looking-at-a-url-but-i-cant-edit-anything", 
            "text": "Make sure you have clicked the big blue button \" Checkout this URL \" near the top. None of the fields can be edited until the URL is checked out.", 
            "title": "1) I'm looking at a URL, but I can't edit anything!"
        }, 
        {
            "location": "/faq/#2-why-are-urls-that-may-have-already-been-archived-by-ann-arbor-available-to-research", 
            "text": "When selecting a URL to review \" 0 \" is the  default  priority; it generally means that no-one has reviewed it  UNLESS  it says  MAY HAVE BEEN HARVESTED AT ANN ARBOR .  In those cases, assign the priority to \" 1 \", so the URL drops down in the queue and then  SKIP IT .", 
            "title": "2) Why are URLs that may have already been archived by Ann Arbor available to research?"
        }, 
        {
            "location": "/faq/#3-what-does-it-mean-if-it-says-crawled-by-internet-archive-yes", 
            "text": "\" Crawled by Internet Archive \" means the  page itself  was crawled; it may or may not mean the  dataset  was crawled.  Based on what Heretrix  Can and Can't Crawl , you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as  Do not harvest .", 
            "title": "3) What does it mean if it says Crawled by Internet Archive: Yes?"
        }, 
        {
            "location": "/faq/#4-how-should-i-handle-overly-broad-sites-with-just-a-search-form-eg-noaagov", 
            "text": "In cases like  noaa.gov , you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.  Complete the  Research  section to the best of your abilities, especially the  Recommended Approach for Harvesting Data .", 
            "title": "4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?"
        }, 
        {
            "location": "/faq/#5-do-we-have-a-scripting-system-set-up-preserving-data-or-data-endpoints-that-are-updated-regularly", 
            "text": "Not yet; addressing these datasets is a goal going-forward.  Currently, indicate in the notes in both the  Research  and  Harvest  sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).", 
            "title": "5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?"
        }, 
        {
            "location": "/faq/#6-what-if-i-have-a-site-and-want-to-know-if-it-has-been-crawled-already", 
            "text": "Internet Archive has both a  Wayback Machine Chrome Extension  and  APIs  you can use to check if something has been archived:   Wayback Machine Chrome Extension  You can also check on the Internet Archive site directly at  archive.org/web/  Wayback CDX Server API   There is a  check-ia  script in the  harvesting tools  for batch URLs.", 
            "title": "6) What if I have a site and want to know if it has been crawled already?"
        }, 
        {
            "location": "/faq/#7-what-if-the-site-has-in-fact-been-crawled-well-by-the-internet-archive", 
            "text": "If the site includes only crawlable data, then there is no need to harvest it. These should be marked  Do not harvest  in the  Research  phase.  If the site includes one of the forms of uncrawlable content: \n1) FTP \n2) Many Files \n3) Database \n4) Visualization/Interactive    Then mark accordingly in and harvest the datasets.", 
            "title": "7) What if the site has in fact been crawled well by the Internet Archive?"
        }, 
        {
            "location": "/faq/#8-what-does-it-mean-when-it-says-checking-out-this-url-will-unlock-url-xxx", 
            "text": "That means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.", 
            "title": "8) What does it mean when it says \"checking out this url will unlock url: xxx\"?"
        }, 
        {
            "location": "/faq/#9-what-do-i-do-when-there-is-stuff-listed-in-the-link-url-section", 
            "text": "If there are a bunch of sub-sites listed that are  not  links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.  If the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.", 
            "title": "9) What do I do when there is stuff listed in the Link URL section?"
        }, 
        {
            "location": "/faq/#10-how-do-i-partition-large-parent-urls-eg-to-reduce-the-size-of-the-download-5-gb", 
            "text": "From the  overview pane , click  Add Url  on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.  Check out the parent URL, and under  Research  use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.", 
            "title": "10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download &lt; 5 GB)?"
        }, 
        {
            "location": "/faq/#11-wifi-is-kind-of-slow-are-there-workarounds-for-a-faster-connection", 
            "text": "Do as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something,  ssh  to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.    Tether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!", 
            "title": "11) Wifi is kind of Slow, are there workarounds for a faster connection?"
        }, 
        {
            "location": "/faq/#12-why-cant-i-edit-the-harvesting-section", 
            "text": "Archivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the  Harvesting  section, you will first need to mark  Research  as complete. Look for the checkbox on the right-hand side at the top of the  Research  section. Once you've checked it, make sure to hit  Save .", 
            "title": "12) Why can't I edit the harvesting section?"
        }, 
        {
            "location": "/faq/#13-when-harvesting-why-doesnt-clicking-on-the-download-zipstarter-button-work", 
            "text": "Unfortunately this is a known issue. Make sure you've marked  Research  complete. Try reloading the page, or switching browsers if you can.  The App is not compatible with Safari.", 
            "title": "13) When harvesting, why doesn't clicking on the Download Zipstarter button work?"
        }, 
        {
            "location": "/faq/#14-in-the-research-section-what-are-all-the-checkboxes-for", 
            "text": "Please read the DataRescue Workflow documentation for more info!", 
            "title": "14) In the Research section, what are all the checkboxes for?"
        }, 
        {
            "location": "/faq/#15-i-have-a-process-improvement-that-would-make-this-go-better", 
            "text": "Great! Open an issue in the  archivers.space GitHub repository , or report it in the appropriate channel in your Slack team.", 
            "title": "15) I have a process improvement that would make this go better!"
        }, 
        {
            "location": "/faq/#16-how-do-i-add-a-new-event", 
            "text": "Admins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!", 
            "title": "16) How do I add a new event?"
        }, 
        {
            "location": "/faq/#17-what-is-the-difference-between-crawlable-and-harvested", 
            "text": "Researchers  investigate whether URLs listed in the  Archivers app  need to be manually downloaded (harvested) or if they can be automatically saved (crawled) by the Internet Archive. The URLs listed as  Crawlable  were determined as such during that research phase and are submitted to the Internet Archive, they do not need to be harvested.   These URLs represent only a small portion of those submitted to the Internet Archive from DataRescue events. Most crawlable URLs are identified by  Seeders  at the beginning of the workflow and completely bypass the Archivers app.", 
            "title": "17) What is the difference between Crawlable and Harvested?"
        }
    ]
}